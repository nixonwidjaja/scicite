{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8038802,"sourceType":"datasetVersion","datasetId":4739305}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-10T15:00:48.392318Z","iopub.execute_input":"2024-04-10T15:00:48.392725Z","iopub.status.idle":"2024-04-10T15:00:48.411786Z","shell.execute_reply.started":"2024-04-10T15:00:48.392692Z","shell.execute_reply":"2024-04-10T15:00:48.410876Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"/kaggle/input/scicite/test.jsonl\n/kaggle/input/scicite/train.jsonl\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df = pd.read_json('/kaggle/input/scicite/train.jsonl', lines=True)\nX_train = train_df['string']\ny_train = train_df['label']\n\ntest_df = pd.read_json('/kaggle/input/scicite/test.jsonl', lines=True)\nX_test = test_df['string']\ny_test = test_df['label']\n\nprint(train_df.shape, test_df.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:00:48.413382Z","iopub.execute_input":"2024-04-10T15:00:48.413685Z","iopub.status.idle":"2024-04-10T15:00:48.582640Z","shell.execute_reply.started":"2024-04-10T15:00:48.413657Z","shell.execute_reply":"2024-04-10T15:00:48.581729Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"(8243, 15) (1861, 14)\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:00:48.583896Z","iopub.execute_input":"2024-04-10T15:00:48.584595Z","iopub.status.idle":"2024-04-10T15:00:48.589117Z","shell.execute_reply.started":"2024-04-10T15:00:48.584560Z","shell.execute_reply":"2024-04-10T15:00:48.588125Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torch \nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom sklearn.utils import resample\n\ndef augment_data_multiclass(X, y):\n    df = pd.concat([X, y], axis=1)\n    majority_class_size = df['label'].value_counts().max()\n    upsampled_dataframes = []\n    for class_label in df['label'].unique():\n        class_df = df[df['label'] == class_label]\n        if len(class_df) < majority_class_size:\n            class_df_upsampled = resample(class_df, replace=True, n_samples=majority_class_size, random_state=10)\n            upsampled_dataframes.append(class_df_upsampled)\n        else:\n            upsampled_dataframes.append(class_df)\n    upsampled_df = pd.concat(upsampled_dataframes)\n    return upsampled_df['string'], upsampled_df['label']\n\n# train the model for a given number of epochs\ndef train_model(model, tokenizer, num_epoch, learning_rate, batch_size, X_train, y_train):\n    # Encode the training data\n    encoded_data_train = tokenizer.batch_encode_plus(\n        X_train,\n        add_special_tokens=True, \n        return_attention_mask=True, \n        pad_to_max_length=True, \n        max_length=512, \n        return_tensors='pt'\n    )\n    labels_train = torch.tensor(y_train)\n\n    # Create data loader for training\n    dataset_train = TensorDataset(encoded_data_train['input_ids'], encoded_data_train['attention_mask'], labels_train)\n    dataloader_train = DataLoader(dataset_train, sampler=RandomSampler(dataset_train), batch_size=batch_size)\n\n    # Connect to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Define optimizer for training data\n    optimizer = Adam(model.parameters(), lr=learning_rate)\n\n    # Training loop\n    for epoch in range(num_epoch):\n        model.train()\n\n        curr_total_loss = 0.\n        count = 0\n        \n        for train_batch in dataloader_train:\n            optimizer.zero_grad()\n\n            id, mask, label = train_batch\n            id = id.to(device)\n            mask = mask.to(device)\n            label = label.to(device)\n\n            outputs = model(id, attention_mask=mask, labels=label)\n\n            loss = outputs.loss\n\n            curr_total_loss += loss.item()\n            count += 1\n\n            loss.backward()\n            \n            optimizer.step()\n\n        avg_loss = curr_total_loss / count\n        print(epoch, avg_loss)       \n    \n    return model \n\n# return f1 macro and accuracy of the model\ndef eval_model(model, tokenizer, X_test, y_test):\n    encoded_data_test = tokenizer.batch_encode_plus(\n        X_test,\n        add_special_tokens=True, \n        return_attention_mask=True, \n        pad_to_max_length=True, \n        max_length=512, \n        return_tensors='pt'\n    )\n    labels_test = torch.tensor(y_test)\n\n    # Create data loader for test data\n    batch_size = 16\n    test_dataset = TensorDataset(encoded_data_test['input_ids'], encoded_data_test['attention_mask'], labels_test)\n    test_dataloader = DataLoader(test_dataset, sampler=SequentialSampler(test_dataset), batch_size=batch_size)\n\n    # Connect to GPU if available\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    model.to(device)\n\n    # Evaluate the model\n    model.eval()\n    predictions = []\n    labels = []\n    with torch.no_grad():\n        for test_batch in test_dataloader:\n            id, mask, label = test_batch\n            id = id.to(device)\n            mask = mask.to(device)\n            label = label.to(device)\n\n            outputs = model(id, attention_mask=mask, labels=label)\n            logits = outputs.logits\n            _, prediction  = torch.max(logits, dim=1)\n\n            predictions.extend(prediction.tolist())\n            labels.extend(label.tolist())\n            \n    f1 = f1_score(labels, predictions, average='macro')\n    acc = accuracy_score(labels, predictions)\n    print(f\"f1 = {f1}, accuracy = {acc}\")\n    return f1, acc\n\n\ndef save_model(model, save_path):\n    torch.save(model.state_dict(), save_path)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:00:48.590724Z","iopub.execute_input":"2024-04-10T15:00:48.590990Z","iopub.status.idle":"2024-04-10T15:00:48.611795Z","shell.execute_reply.started":"2024-04-10T15:00:48.590961Z","shell.execute_reply":"2024-04-10T15:00:48.610888Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"X_train, y_train = augment_data_multiclass(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:00:48.614048Z","iopub.execute_input":"2024-04-10T15:00:48.614330Z","iopub.status.idle":"2024-04-10T15:00:48.638535Z","shell.execute_reply.started":"2024-04-10T15:00:48.614307Z","shell.execute_reply":"2024-04-10T15:00:48.637677Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Fit label encoder and transform string column\ny_train = label_encoder.fit_transform(y_train)\ny_test = label_encoder.transform(y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:00:48.639786Z","iopub.execute_input":"2024-04-10T15:00:48.640144Z","iopub.status.idle":"2024-04-10T15:00:48.651707Z","shell.execute_reply.started":"2024-04-10T15:00:48.640111Z","shell.execute_reply":"2024-04-10T15:00:48.650808Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\nmodel_name = 'distilbert-base-uncased'\ntokenizer = DistilBertTokenizerFast.from_pretrained(model_name)\nmodel = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\nmodel = train_model(model, tokenizer, 5, 4e-5, 32, X_train.to_list(), y_train)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:00:48.652855Z","iopub.execute_input":"2024-04-10T15:00:48.653168Z","iopub.status.idle":"2024-04-10T15:32:04.454897Z","shell.execute_reply.started":"2024-04-10T15:00:48.653140Z","shell.execute_reply":"2024-04-10T15:32:04.453922Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stderr","text":"Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","output_type":"stream"},{"name":"stdout","text":"0 0.3687752045707424\n1 0.14153265489659186\n2 0.06408520703437832\n3 0.043669660702360065\n4 0.030346386409380026\n","output_type":"stream"}]},{"cell_type":"code","source":"eval_model(model, tokenizer, X_test.to_list(), y_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T15:32:04.456443Z","iopub.execute_input":"2024-04-10T15:32:04.456875Z","iopub.status.idle":"2024-04-10T15:32:21.659852Z","shell.execute_reply.started":"2024-04-10T15:32:04.456842Z","shell.execute_reply":"2024-04-10T15:32:21.658914Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stdout","text":"f1 = 0.8191607785516828, accuracy = 0.8334228909188608\n","output_type":"stream"},{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"(0.8191607785516828, 0.8334228909188608)"},"metadata":{}}]}]}